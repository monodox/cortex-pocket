import 'dart:async';

// Temporary mock implementation until actual FFI is set up
class LlamaFFI {
  static dynamic loadModel(String path) => null;
  static dynamic newContext(dynamic model, dynamic params) => null;
  static int tokenize(dynamic context, String text, int maxTokens, bool addBos) => 0;
  static int eval(dynamic context) => -1;
  static String getTokenText(dynamic context, int tokenId) => '';
  static void freeContext(dynamic context) {}
  static void freeModel(dynamic model) {}
}

class LlamaModel {}
class LlamaContext {}
class LlamaContextParams {
  late int seed;
  late int nCtx;
  late int nBatch;
  late int nThreads;
  late double temperature;
  late double topP;
}

class LlamaBridge {
  dynamic _model;
  dynamic _context;
  bool _isLoaded = false;
  static int _responseCount = 0;

  bool get isLoaded => _isLoaded;

  Future<bool> loadModel(String modelPath) async {
    try {
      // Mock model loading for development
      await Future.delayed(const Duration(milliseconds: 500));
      _model = 'mock_model';
      _context = 'mock_context';
      _isLoaded = true;
      return true;
    } catch (e) {
      return false;
    }
  }

  Stream<String> generateTokens(String prompt) async* {
    if (!_isLoaded || _context == null) return;

    String response = _generateResponse(prompt.toLowerCase());
    final words = response.split(' ');
    
    for (final word in words) {
      if (word.isNotEmpty) {
        await Future.delayed(const Duration(milliseconds: 30));
        yield '$word ';
      }
    }
  }

  String _generateResponse(String prompt) {
    _responseCount++;
    
    if (prompt.contains('api') || prompt.contains('setup') || prompt.contains('connect') || prompt.contains('remote')) {
      return "**API Setup Guide:**\n\n1. Go to Settings > Remote API\n2. Enter your API key:\n   - OpenAI: sk-...\n   - Gemini: AIza...\n3. Tap Save & Test\n4. Enable Use Remote API toggle\n5. Use cloud/local toggle in chat\n\n**For better AI responses, connect your API key or install a local model!**";
    }
    
    if (prompt.contains('model') || prompt.contains('local') || prompt.contains('download') || prompt.contains('install')) {
      return "**Local Model Setup:**\n\n1. Go to Models screen\n2. Tap Browse Models\n3. Choose model for your device:\n   - 2-4GB RAM: Gemma 2B (Q4)\n   - 4-6GB RAM: Phi-3.5 Mini\n   - 6GB+ RAM: Llama 3.2 3B\n4. Download .gguf file\n5. Place in assets/models/\n6. Load model\n\n**Local models = 100% privacy!**";
    }
    
    if (prompt.contains('help') || prompt.contains('guide') || prompt.contains('how to use')) {
      return "**How to Use Cortex Pocket:**\n\n**Setup Options:**\n- Type 'api setup' - Connect cloud AI\n- Type 'local model' - Install offline AI\n\n**Features:**\n- Personas - Specialized AI modes\n- File Analysis - Upload documents\n- Benchmarks - Test performance\n- Toggle - Switch local/remote\n\n**Get Started:** Connect API or install model for full AI power!";
    }
    
    if (prompt.contains('hello') || prompt.contains('hi') || prompt.contains('hey')) {
      return "**Welcome to Cortex Pocket!**\n\nI'm your AI assistant, but I need setup first:\n\n**Quick Start:**\n- Type 'api setup' for cloud AI\n- Type 'local model' for offline AI\n- Type 'help' for full guide\n\n**Ready to unlock powerful AI assistance?**";
    }
    
    if (prompt.contains('code') || prompt.contains('programming') || prompt.contains('debug')) {
      return "**Code Assistant Ready!**\n\nI can help with:\n- Debug errors & fix bugs\n- Code review & optimization\n- Explain complex algorithms\n- Write functions & classes\n- Best practices & security\n\n**Share your code and I'll analyze it!**\n\n*For advanced coding help, connect API or install local model*";
    }
    
    if (prompt.contains('write') || prompt.contains('document') || prompt.contains('text')) {
      return "**Writing Assistant Active!**\n\nI can help create:\n- Technical documentation\n- Blog posts & articles\n- Code comments & README\n- Reports & summaries\n- Email & messages\n\n**What would you like to write?**\n\n*Connect API for enhanced writing capabilities*";
    }
    
    if (prompt.contains('analyze') || prompt.contains('data') || prompt.contains('review')) {
      return "**Analysis Expert Ready!**\n\nI can analyze:\n- Code quality & performance\n- Data patterns & insights\n- Security vulnerabilities\n- File contents & logs\n- System metrics\n\n**Upload files or share data to analyze!**\n\n*Install local model for private analysis*";
    }
    
    if (prompt.contains('persona') || prompt.contains('mode') || prompt.contains('specialist')) {
      return "**AI Personas Available:**\n\n**Developer** - Code & architecture\n**Security** - Cybersecurity expert\n**Writer** - Documentation & content\n**Analyst** - Data & insights\n**Guide** - App help & support\n\n**Switch personas in the Personas screen!**\n\n*Each persona needs API or local model to work*";
    }
    
    if (prompt.contains('benchmark') || prompt.contains('performance') || prompt.contains('speed')) {
      return "**Performance Benchmarks:**\n\nTest your device:\n- Token generation speed\n- RAM usage monitoring\n- CPU utilization\n- Model load time\n\n**Expected Performance:**\n- Snapdragon 8 Gen 2: 6-12 tokens/sec\n- Tensor G2: 8-15 tokens/sec\n\n*Benchmarks require local model installation*";
    }
    
    // Default responses with setup reminders
    final defaultResponses = [
      "I understand you're asking about '${prompt.split(' ').take(3).join(' ')}'. To give you detailed help, I need either an API connection or local model installed.\n\nType 'api setup' or 'local model' to get started!",
      "That's an interesting question about '${prompt.split(' ').take(2).join(' ')}'. For comprehensive answers, please connect your API key or install a local AI model.\n\nType 'help' for setup instructions!",
      "I'd love to help with '${prompt.split(' ').take(3).join(' ')}', but I need full AI capabilities first.\n\nQuick setup: Type 'api setup' for cloud AI or 'local model' for offline AI!"
    ];
    
    return defaultResponses[_responseCount % defaultResponses.length];
  }

  void dispose() {
    if (_context != null) {
      LlamaFFI.freeContext(_context!);
      _context = null;
    }
    if (_model != null) {
      LlamaFFI.freeModel(_model!);
      _model = null;
    }
    _isLoaded = false;
  }
}